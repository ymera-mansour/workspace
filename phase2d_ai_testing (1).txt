========================================
PHASE 2D - QODER: AI MODELS COMPREHENSIVE TESTING & VALIDATION
========================================

=== YOUR IDENTITY ===
Your name: QODER
Your role: AI Models Testing & Validation Engineer
Your phase: 2D (NEW - AI MODELS VALIDATION)
Your workspace: C:\Users\Mohamed Mansour\Desktop\YmeraRefactor\

=== CONTEXT ===
‚úÖ Phase 2B: API Gateway created
‚úÖ Phase 2C: AI providers integrated
‚ö†Ô∏è MISSING: Comprehensive testing of all AI models, keys, and configurations

**CRITICAL**: Need to test every AI model, every API key, every model variant to identify issues!

=== YOUR MISSION ===
Create a comprehensive testing system that:
1. **Tests ALL AI providers** (Gemini, Groq, Mistral, DeepSeek, AI21, Codestral, OpenRouter, HuggingFace, Manus)
2. **Tests ALL API keys** for each provider (primary + secondary keys)
3. **Tests ALL model variants** for each provider
4. **Validates rate limits** and quota management
5. **Measures performance** (latency, success rate, quality)
6. **Generates detailed reports** with actionable recommendations
7. **Identifies issues** (expired keys, broken endpoints, model deprecations)

=== SOURCE DIRECTORY ===
Location: C:\Users\Mohamed Mansour\Desktop\YmeraRefactor\

You will test:
```
YmeraRefactor\
‚îú‚îÄ‚îÄ core_services\
‚îÇ   ‚îî‚îÄ‚îÄ ai_mcp\
‚îÇ       ‚îú‚îÄ‚îÄ providers\
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ gemini_adapter.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ groq_adapter.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ mistral_adapter.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ deepseek_adapter.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ai21_adapter.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ codestral_adapter.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ openrouter_adapter.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ huggingface_adapter.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ manus_adapter.py
‚îÇ       ‚îî‚îÄ‚îÄ ai_models_manager.py
‚îî‚îÄ‚îÄ .env.unified
```

=== STEP-BY-STEP INSTRUCTIONS ===

## STEP 1: CREATE COMPREHENSIVE TEST FRAMEWORK (20 minutes)

### 1.1 Create Test Configuration

**File: tests/ai_models/test_config.py**
```python
# YMERA Refactoring Project
# Phase: 2D | Agent: qoder | Created: 2024-12-03
# AI Models Testing Configuration

from dataclasses import dataclass
from typing import List, Dict, Any
from enum import Enum

class TestPriority(Enum):
    CRITICAL = "critical"      # Must work (primary providers)
    HIGH = "high"             # Important (secondary providers)
    MEDIUM = "medium"         # Nice to have
    LOW = "low"               # Experimental

@dataclass
class ModelTestConfig:
    """Configuration for testing a specific model"""
    provider: str
    model_name: str
    test_priority: TestPriority
    expected_capabilities: List[str]
    test_prompts: Dict[str, str]
    max_response_time: float  # seconds
    min_success_rate: float   # 0.0 to 1.0

@dataclass
class ProviderTestConfig:
    """Configuration for testing a provider"""
    provider_name: str
    api_keys: List[str]
    models: List[ModelTestConfig]
    rate_limit: int  # requests per minute
    free_tier: bool
    test_priority: TestPriority

# ============================================================================
# TEST CONFIGURATIONS FOR ALL PROVIDERS
# ============================================================================

# Standard test prompts for consistency
STANDARD_TEST_PROMPTS = {
    "simple_math": "What is 2 + 2? Answer with just the number.",
    "code_generation": "Write a Python function to calculate factorial. Just the code, no explanation.",
    "reasoning": "If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Answer yes or no with brief explanation.",
    "creative": "Write a single sentence about artificial intelligence.",
    "multilingual": "Translate 'Hello, how are you?' to Spanish."
}

# ============================================================================
# GOOGLE GEMINI CONFIGURATION
# ============================================================================
GEMINI_TEST_CONFIG = ProviderTestConfig(
    provider_name="gemini",
    api_keys=[
        "GEMINI_API_KEY",
        "GEMINI_API_KEY_2",
        "GEMINI_API_KEY_3",
        "GEMINI_API_KEY_4",
        "GEMINI_API_KEY_5",
        "GEMINI_API_KEY_6",
        "GEMINI_API_KEY_7",
        "GEMINI_API_KEY_8",
        "GEMINI_API_KEY_9",
        "GEMINI_API_KEY_10",
        "GEMINI_API_KEY_11"
    ],
    models=[
        ModelTestConfig(
            provider="gemini",
            model_name="gemini-2.5-flash",
            test_priority=TestPriority.CRITICAL,
            expected_capabilities=["text", "chat", "fast_response"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=3.0,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="gemini",
            model_name="gemini-2.5-pro",
            test_priority=TestPriority.CRITICAL,
            expected_capabilities=["text", "chat", "advanced_reasoning"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="gemini",
            model_name="gemini-2.0-flash",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=3.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="gemini",
            model_name="gemini-1.5-pro",
            test_priority=TestPriority.LOW,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.85
        )
    ],
    rate_limit=360,  # 360 RPM free tier
    free_tier=True,
    test_priority=TestPriority.CRITICAL
)

# ============================================================================
# GROQ CONFIGURATION
# ============================================================================
GROQ_TEST_CONFIG = ProviderTestConfig(
    provider_name="groq",
    api_keys=[
        "GROQ_API_KEY",
        "GROQ_API_KEY_1",
        "GROQ_API_KEY_2",
        "GROQ_API_KEY_3",
        "GROQ_API_KEY_4",
        "GROQ_API_KEY_5",
        "GROQ_API_KEY_6",
        "GROQ_API_KEY_7",
        "GROQ_API_KEY_8"
    ],
    models=[
        ModelTestConfig(
            provider="groq",
            model_name="llama-3.3-70b-versatile",
            test_priority=TestPriority.CRITICAL,
            expected_capabilities=["text", "chat", "ultra_fast"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=1.5,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="groq",
            model_name="llama-3.1-8b-instant",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "ultra_fast"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=1.0,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="groq",
            model_name="mixtral-8x7b-32768",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "long_context"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=2.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="groq",
            model_name="llama-3.2-90b-vision-preview",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "vision", "multimodal"],
            test_prompts={k: v for k, v in STANDARD_TEST_PROMPTS.items() if k != "multilingual"},
            max_response_time=2.5,
            min_success_rate=0.85
        )
    ],
    rate_limit=30,  # 30 RPM free tier
    free_tier=True,
    test_priority=TestPriority.CRITICAL
)

# ============================================================================
# MISTRAL CONFIGURATION
# ============================================================================
MISTRAL_TEST_CONFIG = ProviderTestConfig(
    provider_name="mistral",
    api_keys=[
        "MISTRAL_API_KEY",
        "MISTRAL_API_KEY_1",
        "MISTRAL_API_KEY_2",
        "MISTRAL_API_KEY_3",
        "MISTRAL_API_KEY_4",
        "MISTRAL_API_KEY_5",
        "MISTRAL_API_KEY_6"
    ],
    models=[
        ModelTestConfig(
            provider="mistral",
            model_name="mistral-small-latest",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "efficient"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=3.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="mistral",
            model_name="mistral-medium-latest",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "balanced"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=4.0,
            min_success_rate=0.92
        ),
        ModelTestConfig(
            provider="mistral",
            model_name="mistral-large-latest",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat", "advanced"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.95
        )
    ],
    rate_limit=100,  # 100 RPM free tier
    free_tier=True,
    test_priority=TestPriority.HIGH
)

# ============================================================================
# CODESTRAL CONFIGURATION
# ============================================================================
CODESTRAL_TEST_CONFIG = ProviderTestConfig(
    provider_name="codestral",
    api_keys=[
        "CODESTRAL_API_KEY"
    ],
    models=[
        ModelTestConfig(
            provider="codestral",
            model_name="codestral-latest",
            test_priority=TestPriority.CRITICAL,
            expected_capabilities=["code_generation", "code_completion"],
            test_prompts={
                "code_generation": STANDARD_TEST_PROMPTS["code_generation"],
                "code_completion": "Complete this function: def add(a, b):",
                "code_explanation": "Explain what this code does: for i in range(10): print(i)"
            },
            max_response_time=3.0,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="codestral",
            model_name="codestral-2405",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["code_generation", "code_completion"],
            test_prompts={
                "code_generation": STANDARD_TEST_PROMPTS["code_generation"],
            },
            max_response_time=3.0,
            min_success_rate=0.90
        )
    ],
    rate_limit=100,  # 100 RPM free tier
    free_tier=True,
    test_priority=TestPriority.CRITICAL
)

# ============================================================================
# DEEPSEEK CONFIGURATION
# ============================================================================
DEEPSEEK_TEST_CONFIG = ProviderTestConfig(
    provider_name="deepseek",
    api_keys=[
        "DEEPSEEK_API_KEY",
        "DEEPSEEK_API_KEY_1",
        "DEEPSEEK_API_KEY_2",
        "DEEPSEEK_API_KEY_3",
        "DEEPSEEK_API_KEY_4",
        "DEEPSEEK_API_KEY_5",
        "DEEPSEEK_API_KEY_6"
    ],
    models=[
        ModelTestConfig(
            provider="deepseek",
            model_name="deepseek-chat",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "reasoning"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=4.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="deepseek",
            model_name="deepseek-coder",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["code_generation", "code_analysis"],
            test_prompts={
                "code_generation": STANDARD_TEST_PROMPTS["code_generation"],
                "simple_math": STANDARD_TEST_PROMPTS["simple_math"]
            },
            max_response_time=4.0,
            min_success_rate=0.90
        )
    ],
    rate_limit=200,  # 200 RPM free tier
    free_tier=True,
    test_priority=TestPriority.HIGH
)

# ============================================================================
# AI21 CONFIGURATION
# ============================================================================
AI21_TEST_CONFIG = ProviderTestConfig(
    provider_name="ai21",
    api_keys=[
        "AI21_API_KEY",
        "AI21_API_KEY_2"
    ],
    models=[
        ModelTestConfig(
            provider="ai21",
            model_name="jamba-large-1.7-2025-07",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "reasoning", "math"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="ai21",
            model_name="jamba-mini-1.7-2025-07",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "fast_response"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=3.0,
            min_success_rate=0.85
        )
    ],
    rate_limit=50,
    free_tier=True,
    test_priority=TestPriority.HIGH
)

# ============================================================================
# OPENROUTER CONFIGURATION
# ============================================================================
OPENROUTER_TEST_CONFIG = ProviderTestConfig(
    provider_name="openrouter",
    api_keys=[
        "OPENROUTER_API_KEY"
    ],
    models=[
        ModelTestConfig(
            provider="openrouter",
            model_name="mistralai/mistral-7b-instruct:free",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=4.0,
            min_success_rate=0.85
        ),
        ModelTestConfig(
            provider="openrouter",
            model_name="meta-llama/llama-3.1-8b-instruct:free",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=4.0,
            min_success_rate=0.85
        ),
        ModelTestConfig(
            provider="openrouter",
            model_name="google/gemma-2-9b-it:free",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.80
        )
    ],
    rate_limit=20,
    free_tier=True,
    test_priority=TestPriority.HIGH
)

# ============================================================================
# HUGGINGFACE CONFIGURATION
# ============================================================================
HUGGINGFACE_TEST_CONFIG = ProviderTestConfig(
    provider_name="huggingface",
    api_keys=[
        "HUGGINGFACE_API_KEY"
    ],
    models=[
        ModelTestConfig(
            provider="huggingface",
            model_name="microsoft/Phi-3-mini-4k-instruct",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=10.0,
            min_success_rate=0.75
        ),
        ModelTestConfig(
            provider="huggingface",
            model_name="meta-llama/Llama-3.2-3B-Instruct",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=10.0,
            min_success_rate=0.75
        ),
        ModelTestConfig(
            provider="huggingface",
            model_name="mistralai/Mistral-7B-Instruct-v0.3",
            test_priority=TestPriority.LOW,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=15.0,
            min_success_rate=0.70
        )
    ],
    rate_limit=1000,
    free_tier=True,
    test_priority=TestPriority.MEDIUM
)

# ============================================================================
# MANUS AI CONFIGURATION
# ============================================================================
MANUS_TEST_CONFIG = ProviderTestConfig(
    provider_name="manus",
    api_keys=[
        "MANUS_API_KEY",
        "MANUS_API_KEY_1",
        "MANUS_API_KEY_2",
        "MANUS_API_KEY_3",
        "MANUS_API_KEY_4"
    ],
    models=[
        ModelTestConfig(
            provider="manus",
            model_name="default",  # Manus may have a default model
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.80
        )
    ],
    rate_limit=100,
    free_tier=True,
    test_priority=TestPriority.MEDIUM
)

# ============================================================================
# MASTER TEST CONFIGURATION
# ============================================================================
ALL_PROVIDERS_TEST_CONFIG = [
    GEMINI_TEST_CONFIG,
    GROQ_TEST_CONFIG,
    MISTRAL_TEST_CONFIG,
    CODESTRAL_TEST_CONFIG,
    DEEPSEEK_TEST_CONFIG,
    AI21_TEST_CONFIG,
    OPENROUTER_TEST_CONFIG,
    HUGGINGFACE_TEST_CONFIG,
    MANUS_TEST_CONFIG
]

# Test execution settings
TEST_SETTINGS = {
    "timeout_per_test": 30.0,  # seconds
    "retry_failed_tests": 2,
    "parallel_tests": False,  # Set to True for faster testing (but may hit rate limits)
    "save_responses": True,  # Save actual responses for analysis
    "test_rate_limits": True,  # Test rate limit handling
    "test_error_handling": True,  # Test error scenarios
}
```

### 1.2 Create Core Test Runner

**File: tests/ai_models/ai_model_tester.py**
```python
# YMERA Refactoring Project
# Phase: 2D | Agent: qoder | Created: 2024-12-03
# AI Models Comprehensive Tester

import asyncio
import time
import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import json
import traceback

from test_config import (
    ALL_PROVIDERS_TEST_CONFIG,
    TEST_SETTINGS,
    ProviderTestConfig,
    ModelTestConfig,
    TestPriority
)

@dataclass
class TestResult:
    """Result of a single test"""
    provider: str
    model: str
    api_key_index: int
    test_name: str
    success: bool
    response_time: float
    error: Optional[str]
    response: Optional[str]
    timestamp: str
    
@dataclass
class ModelTestSummary:
    """Summary of tests for a model"""
    provider: str
    model: str
    total_tests: int
    successful_tests: int
    failed_tests: int
    avg_response_time: float
    success_rate: float
    errors: List[str]
    status: str  # "working", "degraded", "failing", "broken"

@dataclass
class KeyTestSummary:
    """Summary of tests for an API key"""
    provider: str
    key_index: int
    key_name: str
    total_tests: int
    successful_tests: int
    failed_tests: int
    status: str  # "valid", "invalid", "rate_limited", "expired", "insufficient_balance"
    errors: List[str]

@dataclass
class ProviderTestSummary:
    """Summary of tests for a provider"""
    provider: str
    total_keys: int
    working_keys: int
    total_models: int
    working_models: int
    overall_success_rate: float
    status: str  # "healthy", "degraded", "failing", "down"
    recommendations: List[str]

class AIModelTester:
    """Comprehensive AI Model Testing System"""
    
    def __init__(self):
        self.results: List[TestResult] = []
        self.start_time = None
        self.end_time = None
        
    async def run_all_tests(self) -> Dict[str, Any]:
        """Run all configured tests"""
        print("="*80)
        print("YMERA AI MODELS COMPREHENSIVE TESTING")
        print("="*80)
        print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Providers to Test: {len(ALL_PROVIDERS_TEST_CONFIG)}")
        print("="*80)
        print()
        
        self.start_time = time.time()
        
        provider_summaries = []
        
        for provider_config in ALL_PROVIDERS_TEST_CONFIG:
            print(f"\n{'='*80}")
            print(f"Testing Provider: {provider_config.provider_name.upper()}")
            print(f"{'='*80}")
            
            provider_summary = await self._test_provider(provider_config)
            provider_summaries.append(provider_summary)
            
            self._print_provider_summary(provider_summary)
        
        self.end_time = time.time()
        
        # Generate final report
        final_report = self._generate_final_report(provider_summaries)
        
        # Save reports
        self._save_reports(final_report, provider_summaries)
        
        return final_report
    
    async def _test_provider(self, config: ProviderTestConfig) -> ProviderTestSummary:
        """Test all keys and models for a provider"""
        
        # Test all API keys
        key_summaries = []
        for key_index, key_name in enumerate(config.api_keys):
            print(f"\nTesting API Key: {key_name}")
            key_summary = await self._test_api_key(config, key_index, key_name)
            key_summaries.append(key_summary)
            print(f"  Status: {key_summary.status} ({key_summary.successful_tests}/{key_summary.total_tests} tests passed)")
        
        # Test all models
        model_summaries = []
        working_key_index = self._get_working_key_index(key_summaries)
        
        if working_key_index is not None:
            for model_config in config.models:
                print(f"\nTesting Model: {model_config.model_name}")
                model_summary = await self._test_model(config, model_config, working_key_index)
                model_summaries.append(model_summary)
                print(f"  Status: {model_summary.status} ({model_summary.success_rate:.1%} success rate)")
        else:
            print("\n‚ö†Ô∏è  No working API keys found. Skipping model tests.")
        
        # Calculate overall statistics
        total_keys = len(key_summaries)
        working_keys = len([k for k in key_summaries if k.status == "valid"])
        total_models = len(model_summaries)
        working_models = len([m for m in model_summaries if m.status in ["working", "degraded"]])
        
        # Calculate overall success rate
        all_results = [r for r in self.results if r.provider == config.provider_name]
        overall_success_rate = (
            sum(1 for r in all_results if r.success) / len(all_results)
            if all_results else 0.0
        )
        
        # Determine overall status
        if working_keys == 0:
            status = "down"
        elif working_models == 0:
            status = "down"
        elif overall_success_rate < 0.5:
            status = "failing"
        elif overall_success_rate < 0.8:
            status = "degraded"
        else:
            status = "healthy"
        
        # Generate recommendations
        recommendations = self._generate_provider_recommendations(
            config, key_summaries, model_summaries, status
        )
        
        return ProviderTestSummary(
            provider=config.provider_name,
            total_keys=total_keys,
            working_keys=working_keys,
            total_models=total_models,
            working_models=working_models,
            overall_success_rate=overall_success_rate,
            status=status,
            recommendations=recommendations
        )
    
    async def _test_api_key(
        self, 
        config: ProviderTestConfig, 
        key_index: int,
        key_name: str
    ) -> KeyTestSummary:
        """Test a single API key"""
        
        # Get API key from environment
        api_key = os.getenv(key_name)
        
        if not api_key:
            return KeyTestSummary(
                provider=config.provider_name,
                key_index=key_index,
                key_name=key_name,
                total_tests=0,
                successful_tests=0,
                failed_tests=0,
                status="missing",
                errors=["API key not found in environment"]
            )
        
        # Test with a simple model (first model in config)
        if not config.models:
            return KeyTestSummary(
                provider=config.provider_name,
                key_index=key_index,
                key_name=key_name,
                total_tests=0,
                successful_tests=0,
                failed_tests=0,
                status="no_models",
                errors=["No models configured for testing"]
            )
        
        test_model = config.models[0]
        test_results = []
        errors = []
        
        # Run a simple test
        try:
            result = await self._execute_single_test(
                provider=config.provider_name,
                model=test_model.model_name,
                api_key=api_key,
                key_index=key_index,
                test_name="key_validation",
                prompt="Say 'OK' if you can read this.",
                timeout=10.0
            )
            test_results.append(result)
            
            if not result.success:
                errors.append(result.error)
        except Exception as e:
            error_msg = str(e)
            errors.append(error_msg)
            
            # Categorize error
            if "401" in error_msg or "invalid" in error_msg.lower():
                status = "invalid"
            elif "429" in error_msg or "rate limit" in error_msg.lower():
                status = "rate_limited"
            elif "403" in error_msg or "forbidden" in error_msg.lower():
                status = "expired"
            elif "balance" in error_msg.lower() or "quota" in error_msg.lower():
                status = "insufficient_balance"
            else:
                status = "error"
            
            return KeyTestSummary(
                provider=config.provider_name,
                key_index=key_index,
                key_name=key_name,
                total_tests=1,
                successful_tests=0,
                failed_tests=1,
                status=status,
                errors=errors
            )
        
        # Determine status
        if test_results and test_results[0].success:
            status = "valid"
        else:
            status = "invalid"
        
        return KeyTestSummary(
            provider=config.provider_name,
            key_index=key_index,
            key_name=key_name,
            total_tests=len(test_results),
            successful_tests=sum(1 for r in test_results if r.success),
            failed_tests=sum(1 for r in test_results if not r.success),
            status=status,
            errors=errors
        )
    
    async def _test_model(
        self,
        provider_config: ProviderTestConfig,
        model_config: ModelTestConfig,
        key_index: int
    ) -> ModelTestSummary:
        """Test a single model with all test prompts"""
        
        api_key = os.getenv(provider_config.api_keys[key_index])
        test_results = []
        errors = []
        
        # Run all configured test prompts
        for test_name, prompt in model_config.test_prompts.items():
            try:
                result = await self._execute_single_test(
                    provider=provider_config.provider_name,
                    model=model_config.model_name,
                    api_key=api_key,
                    key_index=key_index,
                    test_name=test_name,
                    prompt=prompt,
                    timeout=model_config.max_response_time + 5.0
                )
                test_results.append(result)
                self.results.append(result)
                
                if not result.success:
                    errors.append(f"{test_name}: {result.error}")
                
                # Rate limiting delay
                await asyncio.sleep(60 / provider_config.rate_limit if provider_config.rate_limit > 0 else 0.1)
                
            except Exception as e:
                error_msg = f"{test_name}: {str(e)}"
                errors.append(error_msg)
                print(f"    ‚ùå {error_msg}")
        
        # Calculate statistics
        total_tests = len(test_results)
        successful_tests = sum(1 for r in test_results if r.success)
        failed_tests = total_tests - successful_tests
        
        avg_response_time = (
            sum(r.response_time for r in test_results if r.success) / successful_tests
            if successful_tests > 0 else 0.0
        )
        
        success_rate = successful_tests / total_tests if total_tests > 0 else 0.0
        
        # Determine status
        if success_rate >= model_config.min_success_rate:
            status = "working"
        elif success_rate >= 0.5:
            status = "degraded"
        elif success_rate > 0:
            status = "failing"
        else:
            status = "broken"
        
        return ModelTestSummary(
            provider=provider_config.provider_name,
            model=model_config.model_name,
            total_tests=total_tests,
            successful_tests=successful_tests,
            failed_tests=failed_tests,
            avg_response_time=avg_response_time,
            success_rate=success_rate,
            errors=errors,
            status=status
        )
    
    async def _execute_single_test(
        self,
        provider: str,
        model: str,
        api_key: str,
        key_index: int,
        test_name: str,
        prompt: str,
        timeout: float
    ) -> TestResult:
        """Execute a single test against a model"""
        
        start_time = time.time()
        
        try:
            # Import provider adapter dynamically
            adapter = await self._get_provider_adapter(provider, api_key)
            
            # Execute the test
            response = await asyncio.wait_for(
                adapter.chat_completion(
                    messages=[{"role": "user", "content": prompt}],
                    model=model
                ),
                timeout=timeout
            )
            
            response_time = time.time() - start_time
            
            # Extract response text
            response_text = self._extract_response_text(response)
            
            return TestResult(
                provider=provider,
                model=model,
                api_key_index=key_index,
                test_name=test_name,
                success=True,
                response_time=response_time,
                error=None,
                response=response_text[:200] if TEST_SETTINGS["save_responses"] else None,
                timestamp=datetime.now().isoformat()
            )
            
        except asyncio.TimeoutError:
            return TestResult(
                provider=provider,
                model=model,
                api_key_index=key_index,
                test_name=test_name,
                success=False,
                response_time=timeout,
                error=f"Timeout after {timeout}s",
                response=None,
                timestamp=datetime.now().isoformat()
            )
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f"{type(e).__name__}: {str(e)}"
            
            return TestResult(
                provider=provider,
                model=model,
                api_key_index=key_index,
                test_name=test_name,
                success=False,
                response_time=response_time,
                error=error_msg,
                response=None,
                timestamp=datetime.now().isoformat()
            )
    
    async def _get_provider_adapter(self, provider: str, api_key: str):
        """Get the appropriate provider adapter"""
        
        # Dynamic import based on provider
        if provider == "gemini":
            from core_services.ai_mcp.providers.gemini_adapter import GeminiAdapter
            return GeminiAdapter(api_key)
        elif provider == "groq":
            from core_services.ai_mcp.providers.groq_adapter import GroqAdapter
            return GroqAdapter(api_key)
        elif provider == "mistral":
            from core_services.ai_mcp.providers.mistral_adapter import MistralAdapter
            return MistralAdapter(api_key)
        elif provider == "codestral":
            from core_services.ai_mcp.providers.codestral_adapter import CodestralAdapter
            return CodestralAdapter(api_key)
        elif provider == "deepseek":
            from core_services.ai_mcp.providers.deepseek_adapter import DeepSeekAdapter
            return DeepSeekAdapter(api_key)
        elif provider == "ai21":
            from core_services.ai_mcp.providers.ai21_adapter import AI21Adapter
            return AI21Adapter(api_key)
        elif provider == "openrouter":
            from core_services.ai_mcp.providers.openrouter_adapter import OpenRouterAdapter
            return OpenRouterAdapter(api_key)
        elif provider == "huggingface":
            from core_services.ai_mcp.providers.huggingface_adapter import HuggingFaceAdapter
            return HuggingFaceAdapter(api_key)
        elif provider == "manus":
            from core_services.ai_mcp.providers.manus_adapter import ManusAIAdapter
            return ManusAIAdapter(api_key)
        else:
            raise ValueError(f"Unknown provider: {provider}")
    
    def _extract_response_text(self, response: Any) -> str:
        """Extract text from various response formats"""
        
        if isinstance(response, str):
            return response
        elif isinstance(response, dict):
            # Try common response formats
            if "content" in response:
                return response["content"]
            elif "text" in response:
                return response["text"]
            elif "choices" in response and len(response["choices"]) > 0:
                choice = response["choices"][0]
                if "message" in choice:
                    return choice["message"].get("content", "")
                elif "text" in choice:
                    return choice["text"]
        
        return str(response)
    
    def _get_working_key_index(self, key_summaries: List[KeyTestSummary]) -> Optional[int]:
        """Get index of first working key"""
        for summary in key_summaries:
            if summary.status == "valid":
                return summary.key_index
        return None
    
    def _generate_provider_recommendations(
        self,
        config: ProviderTestConfig,
        key_summaries: List[KeyTestSummary],
        model_summaries: List[ModelTestSummary],
        status: str
    ) -> List[str]:
        """Generate recommendations for a provider"""
        
        recommendations = []
        
        # Key-related recommendations
        invalid_keys = [k for k in key_summaries if k.status == "invalid"]
        expired_keys = [k for k in key_summaries if k.status == "expired"]
        balance_keys = [k for k in key_summaries if k.status == "insufficient_balance"]
        
        if invalid_keys:
            recommendations.append(
                f"‚ùå {len(invalid_keys)} API key(s) are invalid. Remove or replace: " +
                ", ".join(k.key_name for k in invalid_keys)
            )
        
        if expired_keys:
            recommendations.append(
                f"‚è∞ {len(expired_keys)} API key(s) expired. Renew: " +
                ", ".join(k.key_name for k in expired_keys)
            )
        
        if balance_keys:
            recommendations.append(
                f"üí≥ {len(balance_keys)} API key(s) have insufficient balance. Top up: " +
                ", ".join(k.key_name for k in balance_keys)
            )
        
        # Model-related recommendations
        broken_models = [m for m in model_summaries if m.status == "broken"]
        failing_models = [m for m in model_summaries if m.status == "failing"]
        
        if broken_models:
            recommendations.append(
                f"üî¥ {len(broken_models)} model(s) completely broken. Check: " +
                ", ".join(m.model for m in broken_models)
            )
        
        if failing_models:
            recommendations.append(
                f"üü° {len(failing_models)} model(s) have low success rate. Monitor: " +
                ", ".join(m.model for m in failing_models)
            )
        
        # Overall status recommendations
        if status == "down":
            recommendations.append(
                "üö® CRITICAL: Provider is completely down. All keys failed or no working models."
            )
        elif status == "failing":
            recommendations.append(
                "‚ö†Ô∏è  WARNING: Provider is experiencing significant issues. Consider using alternative providers."
            )
        elif status == "degraded":
            recommendations.append(
                "üìâ NOTICE: Provider performance is degraded. Some features may not work reliably."
            )
        elif status == "healthy":
            recommendations.append(
                "‚úÖ Provider is healthy and functioning normally."
            )
        
        return recommendations if recommendations else ["No specific recommendations."]
    
    def _generate_final_report(self, provider_summaries: List[ProviderTestSummary]) -> Dict[str, Any]:
        """Generate comprehensive final report"""
        
        total_duration = self.end_time - self.start_time
        
        # Overall statistics
        total_providers = len(provider_summaries)
        healthy_providers = len([p for p in provider_summaries if p.status == "healthy"])
        degraded_providers = len([p for p in provider_summaries if p.status == "degraded"])
        failing_providers = len([p for p in provider_summaries if p.status == "failing"])
        down_providers = len([p for p in provider_summaries if p.status == "down"])
        
        total_keys = sum(p.total_keys for p in provider_summaries)
        working_keys = sum(p.working_keys for p in provider_summaries)
        
        total_models = sum(p.total_models for p in provider_summaries)
        working_models = sum(p.working_models for p in provider_summaries)
        
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.success)
        
        overall_success_rate = successful_tests / total_tests if total_tests > 0 else 0.0
        
        return {
            "test_summary": {
                "start_time": datetime.fromtimestamp(self.start_time).isoformat(),
                "end_time": datetime.fromtimestamp(self.end_time).isoformat(),
                "duration_seconds": round(total_duration, 2),
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": total_tests - successful_tests,
                "overall_success_rate": round(overall_success_rate, 4)
            },
            "provider_statistics": {
                "total_providers": total_providers,
                "healthy": healthy_providers,
                "degraded": degraded_providers,
                "failing": failing_providers,
                "down": down_providers
            },
            "key_statistics": {
                "total_keys": total_keys,
                "working_keys": working_keys,
                "invalid_keys": total_keys - working_keys,
                "working_percentage": round(working_keys / total_keys * 100, 1) if total_keys > 0 else 0
            },
            "model_statistics": {
                "total_models": total_models,
                "working_models": working_models,
                "broken_models": total_models - working_models,
                "working_percentage": round(working_models / total_models * 100, 1) if total_models > 0 else 0
            },
            "provider_summaries": [asdict(p) for p in provider_summaries],
            "critical_issues": self._identify_critical_issues(provider_summaries),
            "recommendations": self._generate_overall_recommendations(provider_summaries)
        }
    
    def _identify_critical_issues(self, provider_summaries: List[ProviderTestSummary]) -> List[Dict[str, str]]:
        """Identify critical issues that need immediate attention"""
        
        issues = []
        
        for provider in provider_summaries:
            if provider.status == "down":
                issues.append({
                    "severity": "CRITICAL",
                    "provider": provider.provider,
                    "issue": "Provider is completely down",
                    "action": "Check API keys and service status immediately"
                })
            elif provider.status == "failing":
                issues.append({
                    "severity": "HIGH",
                    "provider": provider.provider,
                    "issue": f"Provider failing ({provider.overall_success_rate:.1%} success rate)",
                    "action": "Investigate errors and consider backup providers"
                })
            elif provider.working_keys == 0:
                issues.append({
                    "severity": "CRITICAL",
                    "provider": provider.provider,
                    "issue": "No working API keys",
                    "action": "Obtain new valid API keys"
                })
            elif provider.working_models == 0:
                issues.append({
                    "severity": "CRITICAL",
                    "provider": provider.provider,
                    "issue": "No working models",
                    "action": "Check model names and API compatibility"
                })
        
        return issues
    
    def _generate_overall_recommendations(self, provider_summaries: List[ProviderTestSummary]) -> List[str]:
        """Generate overall system recommendations"""
        
        recommendations = []
        
        # Count provider statuses
        healthy = sum(1 for p in provider_summaries if p.status == "healthy")
        total = len(provider_summaries)
        
        if healthy < total * 0.5:
            recommendations.append(
                "üö® URGENT: Less than 50% of providers are healthy. System reliability is at risk."
            )
        elif healthy < total * 0.75:
            recommendations.append(
                "‚ö†Ô∏è  WARNING: Less than 75% of providers are healthy. Consider adding backup providers."
            )
        else:
            recommendations.append(
                "‚úÖ Good provider diversity. System has good redundancy."
            )
        
        # Key management recommendations
        total_keys = sum(p.total_keys for p in provider_summaries)
        working_keys = sum(p.working_keys for p in provider_summaries)
        
        if working_keys < total_keys * 0.7:
            recommendations.append(
                f"üîë {total_keys - working_keys} API keys need attention. Clean up invalid keys."
            )
        
        # Model recommendations
        total_models = sum(p.total_models for p in provider_summaries)
        working_models = sum(p.working_models for p in provider_summaries)
        
        if working_models < total_models * 0.8:
            recommendations.append(
                f"ü§ñ {total_models - working_models} models not working. Update model configurations."
            )
        
        return recommendations
    
    def _print_provider_summary(self, summary: ProviderTestSummary):
        """Print a formatted provider summary"""
        
        status_emoji = {
            "healthy": "‚úÖ",
            "degraded": "üü°",
            "failing": "üî¥",
            "down": "üíÄ"
        }
        
        print(f"\n{status_emoji.get(summary.status, '‚ùì')} Provider Status: {summary.status.upper()}")
        print(f"   API Keys: {summary.working_keys}/{summary.total_keys} working")
        print(f"   Models: {summary.working_models}/{summary.total_models} working")
        print(f"   Success Rate: {summary.overall_success_rate:.1%}")
        print(f"\n   Recommendations:")
        for rec in summary.recommendations:
            print(f"   - {rec}")
    
    def _save_reports(self, final_report: Dict[str, Any], provider_summaries: List[ProviderTestSummary]):
        """Save all reports to files"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = "_reports/ai_models"
        os.makedirs(report_dir, exist_ok=True)
        
        # Save final report
        final_report_path = f"{report_dir}/final_report_{timestamp}.json"
        with open(final_report_path, 'w') as f:
            json.dump(final_report, f, indent=2)
        print(f"\nüìÑ Final report saved: {final_report_path}")
        
        # Save detailed results
        detailed_results_path = f"{report_dir}/detailed_results_{timestamp}.json"
        with open(detailed_results_path, 'w') as f:
            json.dump([asdict(r) for r in self.results], f, indent=2)
        print(f"üìÑ Detailed results saved: {detailed_results_path}")
        
        # Save markdown report
        markdown_report = self._generate_markdown_report(final_report, provider_summaries)
        markdown_path = f"{report_dir}/report_{timestamp}.md"
        with open(markdown_path, 'w') as f:
            f.write(markdown_report)
        print(f"üìÑ Markdown report saved: {markdown_path}")
        
        # Print summary
        print(f"\n{'='*80}")
        print("TESTING COMPLETE")
        print(f"{'='*80}")
        print(f"Total Tests: {final_report['test_summary']['total_tests']}")
        print(f"Success Rate: {final_report['test_summary']['overall_success_rate']:.1%}")
        print(f"Duration: {final_report['test_summary']['duration_seconds']:.1f}s")
        print(f"{'='*80}")
    
    def _generate_markdown_report(self, final_report: Dict, provider_summaries: List[ProviderTestSummary]) -> str:
        """Generate a formatted markdown report"""
        
        md = []
        md.append("# YMERA AI Models Comprehensive Testing Report")
        md.append(f"\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md.append(f"\n**Duration:** {final_report['test_summary']['duration_seconds']:.1f} seconds")
        md.append("\n---\n")
        
        # Executive Summary
        md.append("## Executive Summary\n")
        md.append(f"- **Total Tests:** {final_report['test_summary']['total_tests']}")
        md.append(f"- **Overall Success Rate:** {final_report['test_summary']['overall_success_rate']:.1%}")
        md.append(f"- **Working Providers:** {final_report['provider_statistics']['healthy']}/{final_report['provider_statistics']['total_providers']}")
        md.append(f"- **Working API Keys:** {final_report['key_statistics']['working_keys']}/{final_report['key_statistics']['total_keys']}")
        md.append(f"- **Working Models:** {final_report['model_statistics']['working_models']}/{final_report['model_statistics']['total_models']}")
        md.append("\n")
        
        # Critical Issues
        if final_report['critical_issues']:
            md.append("## üö® Critical Issues\n")
            for issue in final_report['critical_issues']:
                md.append(f"### {issue['severity']}: {issue['provider']}")
                md.append(f"- **Issue:** {issue['issue']}")
                md.append(f"- **Action:** {issue['action']}")
                md.append("")
        
        # Provider Details
        md.append("## Provider Details\n")
        for provider in provider_summaries:
            status_emoji = {"healthy": "‚úÖ", "degraded": "üü°", "failing": "üî¥", "down": "üíÄ"}
            md.append(f"### {status_emoji.get(provider.status, '‚ùì')} {provider.provider.upper()}")
            md.append(f"- **Status:** {provider.status}")
            md.append(f"- **Success Rate:** {provider.overall_success_rate:.1%}")
            md.append(f"- **API Keys:** {provider.working_keys}/{provider.total_keys} working")
            md.append(f"- **Models:** {provider.working_models}/{provider.total_models} working")
            md.append(f"\n**Recommendations:**")
            for rec in provider.recommendations:
                md.append(f"- {rec}")
            md.append("")
        
        # Overall Recommendations
        md.append("## Overall Recommendations\n")
        for rec in final_report['recommendations']:
            md.append(f"- {rec}")
        
        return "\n".join(md)

# Run tests
async def main():
    tester = AIModelTester()
    await tester.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())
```

## STEP 2: CREATE PROVIDER-SPECIFIC TESTS (30 minutes)

Create individual test scripts for each provider to allow focused testing.

**File: tests/ai_models/test_gemini.py**
```python
# Quick test script for Gemini provider only
import asyncio
from ai_model_tester import AIModelTester
from test_config import GEMINI_TEST_CONFIG

async def main():
    tester = AIModelTester()
    summary = await tester._test_provider(GEMINI_TEST_CONFIG)
    tester._print_provider_summary(summary)

if __name__ == "__main__":
    asyncio.run(main())
```

[Create similar files for each provider: test_groq.py, test_mistral.py, test_deepseek.py, etc.]

## STEP 3: CREATE COMPLETION REPORT (15 minutes)

**File: _reports/qoder/phase2d_qoder_YYYYMMDD_HHMMSS.md**

```markdown
# Qoder Phase 2D Completion Report
Phase: 2D | Agent: qoder | Created: [TIMESTAMP]

## Summary
- Created comprehensive AI models testing framework
- Tested [X] providers with [Y] total API keys
- Validated [Z] models across all providers
- Identified [A] critical issues requiring immediate attention

## Testing Statistics
- **Total Tests**: [X]
- **Success Rate**: [Y]%
- **Working Providers**: [Z]/[Total]
- **Working Keys**: [A]/[Total]
- **Working Models**: [B]/[Total]

## Provider Status

### Healthy Providers ‚úÖ
[List providers with "healthy" status]

### Degraded Providers üü°
[List providers with "degraded" status]

### Failing Providers üî¥
[List providers with "failing" status]

### Down Providers üíÄ
[List providers with "down" status]

## Critical Issues Identified

1. [Issue 1]
   - **Severity**: [CRITICAL/HIGH/MEDIUM]
   - **Provider**: [Name]
   - **Details**: [Description]
   - **Action**: [Required action]

## Recommendations

### Immediate Actions (Priority 1)
- [ ] [Action 1]
- [ ] [Action 2]

### Short-term Actions (Priority 2)
- [ ] [Action 1]
- [ ] [Action 2]

### Long-term Improvements (Priority 3)
- [ ] [Action 1]
- [ ] [Action 2]

## Files Generated
- Final Report: _reports/ai_models/final_report_YYYYMMDD_HHMMSS.json
- Detailed Results: _reports/ai_models/detailed_results_YYYYMMDD_HHMMSS.json
- Markdown Report: _reports/ai_models/report_YYYYMMDD_HHMMSS.md

## Next Steps
1. Review detailed reports
2. Fix identified issues
3. Re-run tests to verify fixes
4. Update provider configurations
5. Document working configurations

## Timestamp
[YYYY-MM-DD HH:MM:SS]
```

=== SUCCESS CRITERIA ===

Phase 2D is complete when:
1. ‚úÖ All providers tested
2. ‚úÖ All API keys validated
3. ‚úÖ All models tested
4. ‚úÖ Detailed reports generated
5. ‚úÖ Critical issues identified
6. ‚úÖ Recommendations provided
7. ‚úÖ Completion report saved

=== ESTIMATED TIME ===
Total: ~2 hours
- Test framework: 20 min
- Provider tests: 30 min  
- Completion report: 15 min
- Actual testing: 45 min (depends on number of models/keys)
- Report analysis: 10 min

========================================
END OF PHASE 2D - QODER PROMPT
========================================