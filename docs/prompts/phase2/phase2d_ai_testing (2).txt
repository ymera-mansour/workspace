========================================
PHASE 2D - QODER: AI MODELS COMPREHENSIVE TESTING & VALIDATION
========================================

=== YOUR IDENTITY ===
Your name: QODER
Your role: AI Models Testing & Validation Engineer
Your phase: 2D (NEW - AI MODELS VALIDATION)
Your workspace: C:\Users\Mohamed Mansour\Desktop\YmeraRefactor\

=== CONTEXT ===
✅ Phase 2B: API Gateway created
✅ Phase 2C: AI providers integrated
⚠️ MISSING: Comprehensive testing of all AI models, keys, and configurations

**CRITICAL**: Need to test every AI model, every API key, every model variant to identify issues!

=== YOUR MISSION ===
Create a comprehensive testing system that:
1. **Tests ALL AI providers** (Gemini, Groq, Mistral, DeepSeek, AI21, Codestral, OpenRouter, HuggingFace, Manus)
2. **Tests ALL API keys** for each provider (primary + secondary keys)
3. **Tests ALL model variants** for each provider
4. **Validates rate limits** and quota management
5. **Measures performance** (latency, success rate, quality)
6. **Generates detailed reports** with actionable recommendations
7. **Identifies issues** (expired keys, broken endpoints, model deprecations)

=== SOURCE DIRECTORY ===
Location: C:\Users\Mohamed Mansour\Desktop\YmeraRefactor\

You will test:
```
YmeraRefactor\
├── core_services\
│   └── ai_mcp\
│       ├── providers\
│       │   ├── gemini_adapter.py
│       │   ├── groq_adapter.py
│       │   ├── mistral_adapter.py
│       │   ├── deepseek_adapter.py
│       │   ├── ai21_adapter.py
│       │   ├── codestral_adapter.py
│       │   ├── openrouter_adapter.py
│       │   ├── huggingface_adapter.py
│       │   └── manus_adapter.py
│       └── ai_models_manager.py
└── .env.unified
```

=== STEP-BY-STEP INSTRUCTIONS ===

## STEP 1: CREATE COMPREHENSIVE TEST FRAMEWORK (20 minutes)

### 1.1 Create Test Configuration

**File: tests/ai_models/test_config.py**
```python
# YMERA Refactoring Project
# Phase: 2D | Agent: qoder | Created: 2024-12-03
# AI Models Testing Configuration

from dataclasses import dataclass
from typing import List, Dict, Any
from enum import Enum

class TestPriority(Enum):
    CRITICAL = "critical"      # Must work (primary providers)
    HIGH = "high"             # Important (secondary providers)
    MEDIUM = "medium"         # Nice to have
    LOW = "low"               # Experimental

@dataclass
class ModelTestConfig:
    """Configuration for testing a specific model"""
    provider: str
    model_name: str
    test_priority: TestPriority
    expected_capabilities: List[str]
    test_prompts: Dict[str, str]
    max_response_time: float  # seconds
    min_success_rate: float   # 0.0 to 1.0

@dataclass
class ProviderTestConfig:
    """Configuration for testing a provider"""
    provider_name: str
    api_keys: List[str]
    models: List[ModelTestConfig]
    rate_limit: int  # requests per minute
    free_tier: bool
    test_priority: TestPriority

# ============================================================================
# TEST CONFIGURATIONS FOR ALL PROVIDERS
# ============================================================================

# Standard test prompts for consistency
STANDARD_TEST_PROMPTS = {
    "simple_math": "What is 2 + 2? Answer with just the number.",
    "code_generation": "Write a Python function to calculate factorial. Just the code, no explanation.",
    "reasoning": "If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Answer yes or no with brief explanation.",
    "creative": "Write a single sentence about artificial intelligence.",
    "multilingual": "Translate 'Hello, how are you?' to Spanish."
}

# ============================================================================
# GOOGLE GEMINI CONFIGURATION
# ============================================================================
GEMINI_TEST_CONFIG = ProviderTestConfig(
    provider_name="gemini",
    api_keys=[
        "GEMINI_API_KEY",
        "GEMINI_API_KEY_2",
        "GEMINI_API_KEY_3",
        "GEMINI_API_KEY_4",
        "GEMINI_API_KEY_5",
        "GEMINI_API_KEY_6",
        "GEMINI_API_KEY_7",
        "GEMINI_API_KEY_8",
        "GEMINI_API_KEY_9",
        "GEMINI_API_KEY_10",
        "GEMINI_API_KEY_11"
    ],
    models=[
        ModelTestConfig(
            provider="gemini",
            model_name="gemini-2.5-flash",
            test_priority=TestPriority.CRITICAL,
            expected_capabilities=["text", "chat", "fast_response"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=3.0,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="gemini",
            model_name="gemini-2.5-pro",
            test_priority=TestPriority.CRITICAL,
            expected_capabilities=["text", "chat", "advanced_reasoning"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="gemini",
            model_name="gemini-2.0-flash",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=3.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="gemini",
            model_name="gemini-1.5-pro",
            test_priority=TestPriority.LOW,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.85
        )
    ],
    rate_limit=360,  # 360 RPM free tier
    free_tier=True,
    test_priority=TestPriority.CRITICAL
)

# ============================================================================
# GROQ CONFIGURATION
# ============================================================================
GROQ_TEST_CONFIG = ProviderTestConfig(
    provider_name="groq",
    api_keys=[
        "GROQ_API_KEY",
        "GROQ_API_KEY_1",
        "GROQ_API_KEY_2",
        "GROQ_API_KEY_3",
        "GROQ_API_KEY_4",
        "GROQ_API_KEY_5",
        "GROQ_API_KEY_6",
        "GROQ_API_KEY_7",
        "GROQ_API_KEY_8"
    ],
    models=[
        ModelTestConfig(
            provider="groq",
            model_name="llama-3.3-70b-versatile",
            test_priority=TestPriority.CRITICAL,
            expected_capabilities=["text", "chat", "ultra_fast"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=1.5,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="groq",
            model_name="llama-3.1-8b-instant",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "ultra_fast"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=1.0,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="groq",
            model_name="mixtral-8x7b-32768",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "long_context"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=2.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="groq",
            model_name="llama-3.2-90b-vision-preview",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "vision", "multimodal"],
            test_prompts={k: v for k, v in STANDARD_TEST_PROMPTS.items() if k != "multilingual"},
            max_response_time=2.5,
            min_success_rate=0.85
        )
    ],
    rate_limit=30,  # 30 RPM free tier
    free_tier=True,
    test_priority=TestPriority.CRITICAL
)

# ============================================================================
# MISTRAL CONFIGURATION
# ============================================================================
MISTRAL_TEST_CONFIG = ProviderTestConfig(
    provider_name="mistral",
    api_keys=[
        "MISTRAL_API_KEY",
        "MISTRAL_API_KEY_1",
        "MISTRAL_API_KEY_2",
        "MISTRAL_API_KEY_3",
        "MISTRAL_API_KEY_4",
        "MISTRAL_API_KEY_5",
        "MISTRAL_API_KEY_6"
    ],
    models=[
        ModelTestConfig(
            provider="mistral",
            model_name="mistral-small-latest",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "efficient"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=3.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="mistral",
            model_name="mistral-medium-latest",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "balanced"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=4.0,
            min_success_rate=0.92
        ),
        ModelTestConfig(
            provider="mistral",
            model_name="mistral-large-latest",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat", "advanced"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.95
        )
    ],
    rate_limit=100,  # 100 RPM free tier
    free_tier=True,
    test_priority=TestPriority.HIGH
)

# ============================================================================
# CODESTRAL CONFIGURATION
# ============================================================================
CODESTRAL_TEST_CONFIG = ProviderTestConfig(
    provider_name="codestral",
    api_keys=[
        "CODESTRAL_API_KEY"
    ],
    models=[
        ModelTestConfig(
            provider="codestral",
            model_name="codestral-latest",
            test_priority=TestPriority.CRITICAL,
            expected_capabilities=["code_generation", "code_completion"],
            test_prompts={
                "code_generation": STANDARD_TEST_PROMPTS["code_generation"],
                "code_completion": "Complete this function: def add(a, b):",
                "code_explanation": "Explain what this code does: for i in range(10): print(i)"
            },
            max_response_time=3.0,
            min_success_rate=0.95
        ),
        ModelTestConfig(
            provider="codestral",
            model_name="codestral-2405",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["code_generation", "code_completion"],
            test_prompts={
                "code_generation": STANDARD_TEST_PROMPTS["code_generation"],
            },
            max_response_time=3.0,
            min_success_rate=0.90
        )
    ],
    rate_limit=100,  # 100 RPM free tier
    free_tier=True,
    test_priority=TestPriority.CRITICAL
)

# ============================================================================
# DEEPSEEK CONFIGURATION
# ============================================================================
DEEPSEEK_TEST_CONFIG = ProviderTestConfig(
    provider_name="deepseek",
    api_keys=[
        "DEEPSEEK_API_KEY",
        "DEEPSEEK_API_KEY_1",
        "DEEPSEEK_API_KEY_2",
        "DEEPSEEK_API_KEY_3",
        "DEEPSEEK_API_KEY_4",
        "DEEPSEEK_API_KEY_5",
        "DEEPSEEK_API_KEY_6"
    ],
    models=[
        ModelTestConfig(
            provider="deepseek",
            model_name="deepseek-chat",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat", "reasoning"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=4.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="deepseek",
            model_name="deepseek-coder",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["code_generation", "code_analysis"],
            test_prompts={
                "code_generation": STANDARD_TEST_PROMPTS["code_generation"],
                "simple_math": STANDARD_TEST_PROMPTS["simple_math"]
            },
            max_response_time=4.0,
            min_success_rate=0.90
        )
    ],
    rate_limit=200,  # 200 RPM free tier
    free_tier=True,
    test_priority=TestPriority.HIGH
)

# ============================================================================
# AI21 CONFIGURATION
# ============================================================================
AI21_TEST_CONFIG = ProviderTestConfig(
    provider_name="ai21",
    api_keys=[
        "AI21_API_KEY",
        "AI21_API_KEY_2"
    ],
    models=[
        ModelTestConfig(
            provider="ai21",
            model_name="jamba-large-1.7-2025-07",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "reasoning", "math"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.90
        ),
        ModelTestConfig(
            provider="ai21",
            model_name="jamba-mini-1.7-2025-07",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "fast_response"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=3.0,
            min_success_rate=0.85
        )
    ],
    rate_limit=50,
    free_tier=True,
    test_priority=TestPriority.HIGH
)

# ============================================================================
# OPENROUTER CONFIGURATION
# ============================================================================
OPENROUTER_TEST_CONFIG = ProviderTestConfig(
    provider_name="openrouter",
    api_keys=[
        "OPENROUTER_API_KEY"
    ],
    models=[
        ModelTestConfig(
            provider="openrouter",
            model_name="mistralai/mistral-7b-instruct:free",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=4.0,
            min_success_rate=0.85
        ),
        ModelTestConfig(
            provider="openrouter",
            model_name="meta-llama/llama-3.1-8b-instruct:free",
            test_priority=TestPriority.HIGH,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=4.0,
            min_success_rate=0.85
        ),
        ModelTestConfig(
            provider="openrouter",
            model_name="google/gemma-2-9b-it:free",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.80
        )
    ],
    rate_limit=20,
    free_tier=True,
    test_priority=TestPriority.HIGH
)

# ============================================================================
# HUGGINGFACE CONFIGURATION
# ============================================================================
HUGGINGFACE_TEST_CONFIG = ProviderTestConfig(
    provider_name="huggingface",
    api_keys=[
        "HUGGINGFACE_API_KEY"
    ],
    models=[
        ModelTestConfig(
            provider="huggingface",
            model_name="microsoft/Phi-3-mini-4k-instruct",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=10.0,
            min_success_rate=0.75
        ),
        ModelTestConfig(
            provider="huggingface",
            model_name="meta-llama/Llama-3.2-3B-Instruct",
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=10.0,
            min_success_rate=0.75
        ),
        ModelTestConfig(
            provider="huggingface",
            model_name="mistralai/Mistral-7B-Instruct-v0.3",
            test_priority=TestPriority.LOW,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=15.0,
            min_success_rate=0.70
        )
    ],
    rate_limit=1000,
    free_tier=True,
    test_priority=TestPriority.MEDIUM
)

# ============================================================================
# MANUS AI CONFIGURATION
# ============================================================================
MANUS_TEST_CONFIG = ProviderTestConfig(
    provider_name="manus",
    api_keys=[
        "MANUS_API_KEY",
        "MANUS_API_KEY_1",
        "MANUS_API_KEY_2",
        "MANUS_API_KEY_3",
        "MANUS_API_KEY_4"
    ],
    models=[
        ModelTestConfig(
            provider="manus",
            model_name="default",  # Manus may have a default model
            test_priority=TestPriority.MEDIUM,
            expected_capabilities=["text", "chat"],
            test_prompts=STANDARD_TEST_PROMPTS,
            max_response_time=5.0,
            min_success_rate=0.80
        )
    ],
    rate_limit=100,
    free_tier=True,
    test_priority=TestPriority.MEDIUM
)

# ============================================================================
# MASTER TEST CONFIGURATION
# ============================================================================
ALL_PROVIDERS_TEST_CONFIG = [
    GEMINI_TEST_CONFIG,
    GROQ_TEST_CONFIG,
    MISTRAL_TEST_CONFIG,
    CODESTRAL_TEST_CONFIG,
    DEEPSEEK_TEST_CONFIG,
    AI21_TEST_CONFIG,
    OPENROUTER_TEST_CONFIG,
    HUGGINGFACE_TEST_CONFIG,
    MANUS_TEST_CONFIG
]

# Test execution settings
TEST_SETTINGS = {
    "timeout_per_test": 30.0,  # seconds
    "retry_failed_tests": 2,
    "parallel_tests": False,  # Set to True for faster testing (but may hit rate limits)
    "save_responses": True,  # Save actual responses for analysis
    "test_rate_limits": True,  # Test rate limit handling
    "test_error_handling": True,  # Test error scenarios
}
```

### 1.2 Create Core Test Runner

**File: tests/ai_models/ai_model_tester.py**
```python
# YMERA Refactoring Project
# Phase: 2D | Agent: qoder | Created: 2024-12-03
# AI Models Comprehensive Tester

import asyncio
import time
import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import json
import traceback

from test_config import (
    ALL_PROVIDERS_TEST_CONFIG,
    TEST_SETTINGS,
    ProviderTestConfig,
    ModelTestConfig,
    TestPriority
)

@dataclass
class TestResult:
    """Result of a single test"""
    provider: str
    model: str
    api_key_index: int
    test_name: str
    success: bool
    response_time: float
    error: Optional[str]
    response: Optional[str]
    timestamp: str
    
@dataclass
class ModelTestSummary:
    """Summary of tests for a model"""
    provider: str
    model: str
    total_tests: int
    successful_tests: int
    failed_tests: int
    avg_response_time: float
    success_rate: float
    errors: List[str]
    status: str  # "working", "degraded", "failing", "broken"

@dataclass
class KeyTestSummary:
    """Summary of tests for an API key"""
    provider: str
    key_index: int
    key_name: str
    total_tests: int
    successful_tests: int
    failed_tests: int
    status: str  # "valid", "invalid", "rate_limited", "expired", "insufficient_balance"
    errors: List[str]

@dataclass
class ProviderTestSummary:
    """Summary of tests for a provider"""
    provider: str
    total_keys: int
    working_keys: int
    total_models: int
    working_models: int
    overall_success_rate: float
    status: str  # "healthy", "degraded", "failing", "down"
    recommendations: List[str]

class AIModelTester:
    """Comprehensive AI Model Testing System"""
    
    def __init__(self):
        self.results: List[TestResult] = []
        self.start_time = None
        self.end_time = None
        
    async def run_all_tests(self) -> Dict[str, Any]:
        """Run all configured tests"""
        print("="*80)
        print("YMERA AI MODELS COMPREHENSIVE TESTING")
        print("="*80)
        print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Providers to Test: {len(ALL_PROVIDERS_TEST_CONFIG)}")
        print("="*80)
        print()
        
        self.start_time = time.time()
        
        provider_summaries = []
        
        for provider_config in ALL_PROVIDERS_TEST_CONFIG:
            print(f"\n{'='*80}")
            print(f"Testing Provider: {provider_config.provider_name.upper()}")
            print(f"{'='*80}")
            
            provider_summary = await self._test_provider(provider_config)
            provider_summaries.append(provider_summary)
            
            self._print_provider_summary(provider_summary)
        
        self.end_time = time.time()
        
        # Generate final report
        final_report = self._generate_final_report(provider_summaries)
        
        # Save reports
        self._save_reports(final_report, provider_summaries)
        
        return final_report
    
    async def _test_provider(self, config: ProviderTestConfig) -> ProviderTestSummary:
        """Test all keys and models for a provider"""
        
        # Test all API keys
        key_summaries = []
        for key_index, key_name in enumerate(config.api_keys):
            print(f"\nTesting API Key: {key_name}")
            key_summary = await self._test_api_key(config, key_index, key_name)
            key_summaries.append(key_summary)
            print(f"  Status: {key_summary.status} ({key_summary.successful_tests}/{key_summary.total_tests} tests passed)")
        
        # Test all models
        model_summaries = []
        working_key_index = self._get_working_key_index(key_summaries)
        
        if working_key_index is not None:
            for model_config in config.models:
                print(f"\nTesting Model: {model_config.model_name}")
                model_summary = await self._test_model(config, model_config, working_key_index)
                model_summaries.append(model_summary)
                print(f"  Status: {model_summary.status} ({model_summary.success_rate:.1%} success rate)")
        else:
            print("\n⚠️  No working API keys found. Skipping model tests.")
        
        # Calculate overall statistics
        total_keys = len(key_summaries)
        working_keys = len([k for k in key_summaries if k.status == "valid"])
        total_models = len(model_summaries)
        working_models = len([m for m in model_summaries if m.status in ["working", "degraded"]])
        
        # Calculate overall success rate
        all_results = [r for r in self.results if r.provider == config.provider_name]
        overall_success_rate = (
            sum(1 for r in all_results if r.success) / len(all_results)
            if all_results else 0.0
        )
        
        # Determine overall status
        if working_keys == 0:
            status = "down"
        elif working_models == 0:
            status = "down"
        elif overall_success_rate < 0.5:
            status = "failing"
        elif overall_success_rate < 0.8:
            status = "degraded"
        else:
            status = "healthy"
        
        # Generate recommendations
        recommendations = self._generate_provider_recommendations(
            config, key_summaries, model_summaries, status
        )
        
        return ProviderTestSummary(
            provider=config.provider_name,
            total_keys=total_keys,
            working_keys=working_keys,
            total_models=total_models,
            working_models=working_models,
            overall_success_rate=overall_success_rate,
            status=status,
            recommendations=recommendations
        )
    
    async def _test_api_key(
        self, 
        config: ProviderTestConfig, 
        key_index: int,
        key_name: str
    ) -> KeyTestSummary:
        """Test a single API key"""
        
        # Get API key from environment
        api_key = os.getenv(key_name)
        
        if not api_key:
            return KeyTestSummary(
                provider=config.provider_name,
                key_index=key_index,
                key_name=key_name,
                total_tests=0,
                successful_tests=0,
                failed_tests=0,
                status="missing",
                errors=["API key not found in environment"]
            )
        
        # Test with a simple model (first model in config)
        if not config.models:
            return KeyTestSummary(
                provider=config.provider_name,
                key_index=key_index,
                key_name=key_name,
                total_tests=0,
                successful_tests=0,
                failed_tests=0,
                status="no_models",
                errors=["No models configured for testing"]
            )
        
        test_model = config.models[0]
        test_results = []
        errors = []
        
        # Run a simple test
        try:
            result = await self._execute_single_test(
                provider=config.provider_name,
                model=test_model.model_name,
                api_key=api_key,
                key_index=key_index,
                test_name="key_validation",
                prompt="Say 'OK' if you can read this.",
                timeout=10.0
            )
            test_results.append(result)
            
            if not result.success:
                errors.append(result.error)
        except Exception as e:
            error_msg = str(e)
            errors.append(error_msg)
            
            # Categorize error
            if "401" in error_msg or "invalid" in error_msg.lower():
                status = "invalid"
            elif "429" in error_msg or "rate limit" in error_msg.lower():
                status = "rate_limited"
            elif "403" in error_msg or "forbidden" in error_msg.lower():
                status = "expired"
            elif "balance" in error_msg.lower() or "quota" in error_msg.lower():
                status = "insufficient_balance"
            else:
                status = "error"
            
            return KeyTestSummary(
                provider=config.provider_name,
                key_index=key_index,
                key_name=key_name,
                total_tests=1,
                successful_tests=0,
                failed_tests=1,
                status=status,
                errors=errors
            )
        
        # Determine status
        if test_results and test_results[0].success:
            status = "valid"
        else:
            status = "invalid"
        
        return KeyTestSummary(
            provider=config.provider_name,
            key_index=key_index,
            key_name=key_name,
            total_tests=len(test_results),
            successful_tests=sum(1 for r in test_results if r.success),
            failed_tests=sum(1 for r in test_results if not r.success),
            status=status,
            errors=errors
        )
    
    async def _test_model(
        self,
        provider_config: ProviderTestConfig,
        model_config: ModelTestConfig,
        key_index: int
    ) -> ModelTestSummary:
        """Test a single model with all test prompts"""
        
        api_key = os.getenv(provider_config.api_keys[key_index])
        test_results = []
        errors = []
        
        # Run all configured test prompts
        for test_name, prompt in model_config.test_prompts.items():
            try:
                result = await self._execute_single_test(
                    provider=provider_config.provider_name,
                    model=model_config.model_name,
                    api_key=api_key,
                    key_index=key_index,
                    test_name=test_name,
                    prompt=prompt,
                    timeout=model_config.max_response_time + 5.0
                )
                test_results.append(result)
                self.results.append(result)
                
                if not result.success:
                    errors.append(f"{test_name}: {result.